{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b13d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec52dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366e606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e180e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca96424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38844602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"hackathon_attendance_model.h5\")\n",
    "\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Hackathon Attendance Prediction\")\n",
    "\n",
    "# User Input Form\n",
    "with st.form(\"attendance_form\"):\n",
    "    downloaded_tickets = st.selectbox(\"Downloaded Tickets\", [0, 1])\n",
    "    engagement_level = st.slider(\"Engagement Level\", 0, 1,2,3)\n",
    "    interested_in_job = st.selectbox(\"Interested in Job Opportunities?\", [0, 1])\n",
    "    long_distance_travel = st.selectbox(\"Long Distance Travel?\", [0, 1])\n",
    "    questionnaire = st.selectbox(\"Filled the Questionnaire?\", [\"Yes\", \"No\"])\n",
    "\n",
    "    submit = st.form_submit_button(\"Predict Attendance\")\n",
    "\n",
    "# Preprocess and Predict\n",
    "if submit:\n",
    "    # Convert categorical 'Yes/No' to numerical (Yes = 1, No = 0)\n",
    "    questionnaire_num = 1 if questionnaire == \"Yes\" else 0\n",
    "\n",
    "    # Prepare input array\n",
    "    user_input = np.array([[downloaded_tickets, engagement_level, interested_in_job, long_distance_travel, questionnaire_num]])\n",
    "\n",
    "    # Scale the input data\n",
    "    user_input_scaled = scaler.transform(user_input)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(user_input_scaled)\n",
    "    predicted_class = int(prediction > 0.5)\n",
    "\n",
    "    # Show Prediction\n",
    "    if predicted_class == 1:\n",
    "        st.success(\"✅ The person is **likely to ATTEND** the hackathon!\")\n",
    "    else:\n",
    "        st.error(\"❌ The person is **NOT likely to attend** the hackathon.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb730bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb115d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac6c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ecbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcbb976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe6f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a04b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2bfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81e0f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 1s 10ms/step - loss: 0.7972 - accuracy: 0.5500 - val_loss: 0.6310 - val_accuracy: 0.6700\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6344 - accuracy: 0.7025 - val_loss: 0.6043 - val_accuracy: 0.6900\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5586 - accuracy: 0.7175 - val_loss: 0.5930 - val_accuracy: 0.7100\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5534 - accuracy: 0.7100 - val_loss: 0.5825 - val_accuracy: 0.7200\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5346 - accuracy: 0.7450 - val_loss: 0.5743 - val_accuracy: 0.7300\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4788 - accuracy: 0.7525 - val_loss: 0.5698 - val_accuracy: 0.6900\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4816 - accuracy: 0.7550 - val_loss: 0.5587 - val_accuracy: 0.7300\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4823 - accuracy: 0.7650 - val_loss: 0.5503 - val_accuracy: 0.7300\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5111 - accuracy: 0.7350 - val_loss: 0.5522 - val_accuracy: 0.7300\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4763 - accuracy: 0.7500 - val_loss: 0.5509 - val_accuracy: 0.7500\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7400 - val_loss: 0.5551 - val_accuracy: 0.7500\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4772 - accuracy: 0.7500 - val_loss: 0.5475 - val_accuracy: 0.7500\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4668 - accuracy: 0.7650 - val_loss: 0.5323 - val_accuracy: 0.7500\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4575 - accuracy: 0.7775 - val_loss: 0.5214 - val_accuracy: 0.7700\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4957 - accuracy: 0.7300 - val_loss: 0.5183 - val_accuracy: 0.7400\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4943 - accuracy: 0.7575 - val_loss: 0.5152 - val_accuracy: 0.7300\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4989 - accuracy: 0.7325 - val_loss: 0.5213 - val_accuracy: 0.7300\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7450 - val_loss: 0.5202 - val_accuracy: 0.7400\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4511 - accuracy: 0.7650 - val_loss: 0.5049 - val_accuracy: 0.7400\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4987 - accuracy: 0.7625 - val_loss: 0.4999 - val_accuracy: 0.7300\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4982 - accuracy: 0.7625 - val_loss: 0.5011 - val_accuracy: 0.7300\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4509 - accuracy: 0.7850 - val_loss: 0.5035 - val_accuracy: 0.7400\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4681 - accuracy: 0.7675 - val_loss: 0.4870 - val_accuracy: 0.7400\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4577 - accuracy: 0.7825 - val_loss: 0.4790 - val_accuracy: 0.7400\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.7925 - val_loss: 0.4756 - val_accuracy: 0.7400\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4486 - accuracy: 0.7775 - val_loss: 0.4672 - val_accuracy: 0.7400\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4519 - accuracy: 0.7725 - val_loss: 0.4636 - val_accuracy: 0.7400\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4675 - accuracy: 0.7650 - val_loss: 0.4684 - val_accuracy: 0.7200\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4885 - accuracy: 0.7500 - val_loss: 0.4656 - val_accuracy: 0.7400\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.7800 - val_loss: 0.4674 - val_accuracy: 0.7400\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4837 - accuracy: 0.7825 - val_loss: 0.4653 - val_accuracy: 0.7400\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4282 - accuracy: 0.7825 - val_loss: 0.4734 - val_accuracy: 0.7400\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4731 - accuracy: 0.7775 - val_loss: 0.4793 - val_accuracy: 0.7400\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4669 - accuracy: 0.7600 - val_loss: 0.4723 - val_accuracy: 0.7400\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4382 - accuracy: 0.7875 - val_loss: 0.4665 - val_accuracy: 0.7200\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4400 - accuracy: 0.7700 - val_loss: 0.4670 - val_accuracy: 0.7200\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4339 - accuracy: 0.7650 - val_loss: 0.4654 - val_accuracy: 0.7400\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Improved Neural Network Accuracy: 74.00%\n",
      "Classification Report for Neural Network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80        62\n",
      "           1       0.68      0.61      0.64        38\n",
      "\n",
      "    accuracy                           0.74       100\n",
      "   macro avg       0.72      0.71      0.72       100\n",
      "weighted avg       0.74      0.74      0.74       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization  # Import BatchNormalization here\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 500 rows of synthetic data\n",
    "n_samples = 500\n",
    "\n",
    "# Generate features with realistic distributions\n",
    "person_ids = np.arange(1, n_samples + 1)\n",
    "registered = np.random.binomial(1, 0.65, n_samples)  # 65% registered\n",
    "\n",
    "# Create engagement level based on registration\n",
    "# If not registered, engagement level is 0\n",
    "engagement_level = np.zeros(n_samples, dtype=int)\n",
    "for i in range(n_samples):\n",
    "    if registered[i] == 1:  # If registered\n",
    "        engagement_level[i] = np.random.choice([1, 2, 3], p=[0.3, 0.5, 0.2])  # Distribution for registered users\n",
    "\n",
    "# Set other features (engagement, job interest, long distance, questionnaire, hackathon attendance) for unregistered users to 0\n",
    "interested_in_job = np.where(registered == 1, np.random.binomial(1, 0.60, n_samples), 0)\n",
    "long_distance = np.where(registered == 1, np.random.binomial(1, 0.70, n_samples), 0)\n",
    "\n",
    "# Generate questionnaire completion with bias toward registered users\n",
    "questionnaire = np.zeros(n_samples, dtype=int)\n",
    "for i in range(n_samples):\n",
    "    if registered[i] == 1:  # If registered\n",
    "        questionnaire_prob = 0.7  # 70% of registered users complete the questionnaire\n",
    "        questionnaire[i] = np.random.binomial(1, questionnaire_prob)\n",
    "\n",
    "# Previous hackathon attendance with bias toward engaged users\n",
    "prev_hack = np.zeros(n_samples, dtype=int)\n",
    "for i in range(n_samples):\n",
    "    if registered[i] == 1:  # If registered\n",
    "        prev_hack_prob = 0.2 + 0.1 * engagement_level[i]  # Base 20% plus 10% per engagement level\n",
    "        prev_hack_prob = min(prev_hack_prob, 0.8)  # Cap at 80% probability\n",
    "        prev_hack[i] = np.random.binomial(1, prev_hack_prob)\n",
    "\n",
    "# Create dataframe\n",
    "data = pd.DataFrame({\n",
    "    'PersonId': person_ids,\n",
    "    'Registered': registered,\n",
    "    'Engagement_Level': engagement_level,\n",
    "    'Interested_In_Job_Opp': interested_in_job,\n",
    "    'Long_Distance': long_distance,\n",
    "    'Questionnaire': questionnaire,\n",
    "    'Prev_Hack': prev_hack\n",
    "})\n",
    "\n",
    "# Create a more realistic target variable based on features\n",
    "attendance_prob = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    if registered[i] == 0:\n",
    "        # Unregistered users have very low chance of attendance\n",
    "        attendance_prob[i] = 0.02  # 2% base probability for unregistered\n",
    "    else:\n",
    "        # For registered users, calculate based on other factors\n",
    "        attendance_prob[i] = (\n",
    "            0.15 +  # base probability for registered users\n",
    "            0.15 * (engagement_level[i] / 3) +  # higher engagement increases probability\n",
    "            0.05 * interested_in_job[i] +  # slight boost for job interest\n",
    "            -0.10 * long_distance[i] +  # long distance decreases probability\n",
    "            0.20 * questionnaire[i] +  # completing questionnaire increases probability\n",
    "            0.25 * prev_hack[i]  # previous attendance is a strong predictor\n",
    "        )\n",
    "\n",
    "# Add some random noise to make the relationship less deterministic\n",
    "noise = np.random.normal(0, 0.05, n_samples)\n",
    "attendance_prob += noise\n",
    "\n",
    "# Ensure probabilities are between 0 and 1\n",
    "attendance_prob = np.clip(attendance_prob, 0.01, 0.99)\n",
    "\n",
    "# Generate Final_Attendance based on calculated probabilities\n",
    "data['Final_Attendance'] = np.random.binomial(1, attendance_prob, n_samples)\n",
    "\n",
    "# Convert boolean columns to Yes/No for consistency with your example\n",
    "boolean_columns = ['Registered', 'Interested_In_Job_Opp', 'Long_Distance', \n",
    "                   'Questionnaire', 'Prev_Hack', 'Final_Attendance']\n",
    "\n",
    "for col in boolean_columns:\n",
    "    data[col] = np.where(data[col] == 1, 'Yes', 'No')\n",
    "\n",
    "# Convert categorical 'Yes'/'No' to 1/0 for scaling\n",
    "data[boolean_columns] = data[boolean_columns].applymap(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Save the dataset to CSV\n",
    "data.to_csv('hackathon_attendance_dataset.csv', index=False)\n",
    "\n",
    "# Now read the dataset back (this is the critical step to use the saved data)\n",
    "data = pd.read_csv('hackathon_attendance_dataset.csv')\n",
    "\n",
    "# Prepare data for training\n",
    "X = data.drop(columns=['PersonId', 'Final_Attendance'])\n",
    "y = data['Final_Attendance']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer and first Hidden Layer with Batch Normalization\n",
    "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())  # Add Batch Normalization\n",
    "model.add(Dropout(0.3))  # Dropout layer\n",
    "# Second Hidden Layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))  # Dropout layer\n",
    "# Second Hidden Layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))  # Dropout layer\n",
    "\n",
    "# Third Hidden Layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))  # Dropout layer\n",
    "\n",
    "# Output Layer (1 neuron, sigmoid activation)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, \n",
    "                    validation_data=(X_test_scaled, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_nn = (model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "print(f\"Improved Neural Network Accuracy: {accuracy_nn * 100:.2f}%\")\n",
    "print(\"Classification Report for Neural Network:\")\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa29d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
